{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt, gridspec\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, trange\n",
    "from pyDOE import lhs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "seed = 1234\n",
    "torch.set_default_dtype(torch.float)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('GPU:', use_gpu)\n",
    "adam_lr, LBFGS_lr = 0.001, 0.5\n",
    "adam_iter, LBFGS_iter = 5000, 50000\n",
    "AM_count = 10\n",
    "AM_type = 0\n",
    "AM_K = 1\n",
    "M = 500\n",
    "N = 1500\n",
    "AW_lr = 0.001\n",
    "model_type = 2\n",
    "lb = np.array([0, 0, 0])\n",
    "ub = np.array([2*np.pi, 2*np.pi, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_fun(lb, ub, num):\n",
    "    temp = torch.from_numpy(lb + (ub - lb) * lhs(3, num)).float()\n",
    "    if use_gpu:\n",
    "        temp = temp.cuda()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cuda(data):\n",
    "    if use_gpu:\n",
    "        data = data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(Net1, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.iter = 0\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linear = nn.ModuleList([nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])\n",
    "        for i in range(len(layers) - 1):\n",
    "            nn.init.xavier_normal_(self.linear[i].weight.data, gain=1.0)\n",
    "            nn.init.zeros_(self.linear[i].bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        a = self.activation(self.linear[0](x))\n",
    "        for i in range(1, len(self.layers) - 2):\n",
    "            z = self.linear[i](a)\n",
    "            a = self.activation(z)\n",
    "        a = self.linear[-1](a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(Net2, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.iter = 0\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linear = nn.ModuleList([nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])\n",
    "        for i in range(len(layers) - 1):\n",
    "            nn.init.xavier_normal_(self.linear[i].weight.data, gain=1.0)\n",
    "            nn.init.zeros_(self.linear[i].bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        a = self.activation(self.linear[0](x))\n",
    "        for i in range(1, len(self.layers) - 2):\n",
    "            z = self.linear[i](a)\n",
    "            a = self.activation(z)\n",
    "        a = self.linear[-1](a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, net1,net2,x_bc,u_bc,v_bc,x_ic,u_ic,v_ic,\n",
    "                 x_f_loss_fun,x_test,x_test_exact_u,x_test_exact_v):\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "        self.x_ic_s = None\n",
    "        self.x_bc_s = None\n",
    "        self.x_f_s = None\n",
    "        self.x_f_N = None\n",
    "        self.x_f_M = None\n",
    "        self.x_f_loss_fun = x_f_loss_fun\n",
    "        self.x_ic = x_ic\n",
    "        self.u_ic = u_ic\n",
    "        self.v_ic = v_ic\n",
    "        self.x_bc = x_bc\n",
    "        self.u_bc = u_bc\n",
    "        self.v_bc = v_bc\n",
    "        self.optimizer_LBFGS_u = None\n",
    "        self.optimizer_LBFGS_v = None\n",
    "        self.x_test = x_test\n",
    "        self.x_test_exact_u = x_test_exact_u\n",
    "        self.x_test_exact_v = x_test_exact_v\n",
    "        self.x_f_N = None\n",
    "        self.x_f_M = None\n",
    "        self.start_loss_collect = False\n",
    "        self.x_label_loss_collect = []\n",
    "        self.x_f_loss_collect = []\n",
    "        self.x_test_estimate_collect = []\n",
    "        self.s_collect = []\n",
    "             \n",
    "    def train_U(self,x):\n",
    "        return self.net1(x)\n",
    "    \n",
    "    def train_V(self,x):\n",
    "        return self.net2(x)\n",
    "    \n",
    "    def predict_U(self, x):\n",
    "        return self.train_U(x)\n",
    "    \n",
    "    def predict_V(self, x):\n",
    "        return self.train_V(x)\n",
    "    \n",
    "    def likelihood_loss(self, loss_e, loss_ic, loss_bc):\n",
    "        loss = torch.exp(-self.x_f_s) * loss_e.detach() + self.x_f_s \\\n",
    "               + torch.exp(-self.x_ic_s) * loss_ic.detach() + self.x_ic_s \\\n",
    "               + torch.exp(-self.x_bc_s) * loss_bc.detach() + self.x_bc_s\n",
    "        return loss\n",
    "    \n",
    "    def true_loss(self, loss_e, loss_ic, loss_bc):\n",
    "        return torch.exp(-self.x_f_s.detach()) * loss_e + torch.exp(-self.x_ic_s.detach()) * loss_ic + torch.exp(\n",
    "            -self.x_bc_s.detach()) * loss_bc\n",
    "    \n",
    "    # 计算迭代损失\n",
    "    def epoch_loss(self):\n",
    "        x_f = torch.cat((self.x_f_N,self.x_f_M), dim=0)\n",
    "        f1, f2 = self.x_f_loss_fun(x_f,self.train_U,self.train_V)\n",
    "        loss_equation = torch.mean(f1 ** 2 + f2 ** 2)\n",
    "        \n",
    "        loss_ic = torch.mean((self.train_U(self.x_ic) - self.u_ic) ** 2 + (self.train_V(self.x_ic) - self.v_ic) ** 2)\n",
    "        \n",
    "        loss_bc = torch.mean((self.train_U(self.x_bc) - self.u_bc) ** 2 + (self.train_V(self.x_bc) - self.v_bc) ** 2)\n",
    "        # 控制方程损失，初始条件损失以及边界条件损失\n",
    "        return loss_equation, loss_ic, loss_bc\n",
    "    \n",
    "    # LBFGS迭代损失函数\n",
    "    def LBFGS_epoch_loss(self):\n",
    "        self.optimizer_LBFGS_u.zero_grad()\n",
    "        self.optimizer_LBFGS_v.zero_grad()\n",
    "        x_f = torch.cat((self.x_f_N, self.x_f_M), dim=0)\n",
    "        f1, f2 = self.x_f_loss_fun(x_f,self.train_U,self.train_V)\n",
    "        loss_equation = torch.mean(f1 ** 2 + f2 ** 2)\n",
    "        loss_ic = torch.mean((self.train_U(self.x_ic) - self.u_ic) ** 2 + (self.train_V(self.x_ic) - self.v_ic) ** 2)\n",
    "        \n",
    "        loss_bc = torch.mean((self.train_U(self.x_bc) - self.u_bc) ** 2 + (self.train_V(self.x_bc) - self.v_bc) ** 2)\n",
    "        loss = self.true_loss(loss_equation, loss_ic, loss_bc)\n",
    "        loss.backward()\n",
    "        self.net1.iter += 1\n",
    "        self.net2.iter += 1\n",
    "        # print('Iter:', self.net1.iter, 'Loss:', loss.item())\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self):\n",
    "        pred_u = self.train_U(self.x_test).cpu().detach().numpy()\n",
    "        pred_v = self.train_V(self.x_test).cpu().detach().numpy()\n",
    "        exact_u = self.x_test_exact_u.cpu().detach().numpy()\n",
    "        exact_v = self.x_test_exact_v.cpu().detach().numpy()\n",
    "        error_u = np.linalg.norm(pred_u - exact_u, 2) / np.linalg.norm(exact_u, 2)\n",
    "        error_v = np.linalg.norm(pred_v - exact_v, 2) / np.linalg.norm(exact_v, 2)\n",
    "        return error_u,error_v\n",
    "    \n",
    "    def run_baseline(self):\n",
    "        optimizer_adam_u = torch.optim.Adam(self.net1.parameters(), lr = adam_lr)\n",
    "        optimizer_adam_v = torch.optim.Adam(self.net2.parameters(), lr = adam_lr)\n",
    "        self.optimizer_LBFGS_u = torch.optim.LBFGS(self.net1.parameters(),lr = LBFGS_lr,\n",
    "                                                 max_iter=LBFGS_iter)\n",
    "        self.optimizer_LBFGS_v = torch.optim.LBFGS(self.net2.parameters(),lr = LBFGS_lr,\n",
    "                                                 max_iter=LBFGS_iter)\n",
    "        pbar = trange(adam_iter, ncols=100)\n",
    "        for i in pbar:\n",
    "            optimizer_adam_u.zero_grad()\n",
    "            optimizer_adam_v.zero_grad()\n",
    "            loss_e,loss_ic,loss_bc = self.epoch_loss()\n",
    "            loss = self.true_loss(loss_e, loss_ic, loss_bc)\n",
    "            loss.backward()\n",
    "            optimizer_adam_u.step()\n",
    "            optimizer_adam_v.step()\n",
    "            self.net1.iter += 1\n",
    "            self.net2.iter += 1\n",
    "            pbar.set_postfix({'Iter':self.net1.iter,\n",
    "                              'Loss':'{0:.2e}'.format(loss.item())\n",
    "                              }) \n",
    "        print('Adam done!')\n",
    "        #这一块可能有问题，到时候再看\n",
    "        self.optimizer_LBFGS_u.step(self.LBFGS_epoch_loss)\n",
    "        self.optimizer_LBFGS_v.step(self.LBFGS_epoch_loss)\n",
    "        print('LBFGS done!')\n",
    "        \n",
    "        error_u, error_v = self.evaluate()\n",
    "        print('Test_L2error_u:', '{0:.2e}'.format(error_u))\n",
    "        print('Test_L2error_v:', '{0:.2e}'.format(error_v))\n",
    "        \n",
    "    def run_AM(self):\n",
    "        for move_count in range(AM_count):\n",
    "            optimizer_adam_u = torch.optim.Adam(self.net1.parameters(), lr = adam_lr)\n",
    "            optimizer_adam_v = torch.optim.Adam(self.net2.parameters(), lr = adam_lr)\n",
    "            self.optimizer_LBFGS_u = torch.optim.LBFGS(self.net1.parameters(),lr = LBFGS_lr,\n",
    "                                                    max_iter=LBFGS_iter)\n",
    "            self.optimizer_LBFGS_v = torch.optim.LBFGS(self.net2.parameters(),lr = LBFGS_lr,\n",
    "                                                    max_iter=LBFGS_iter)\n",
    "            pbar = trange(adam_iter, ncols=100)\n",
    "            for i in pbar:\n",
    "                optimizer_adam_u.zero_grad()\n",
    "                optimizer_adam_v.zero_grad()\n",
    "                loss_e,loss_ic,loss_bc = self.epoch_loss()\n",
    "                loss = self.true_loss(loss_e, loss_ic, loss_bc)\n",
    "                loss.backward()\n",
    "                optimizer_adam_u.step()\n",
    "                optimizer_adam_v.step()\n",
    "                self.net1.iter += 1\n",
    "                self.net2.iter += 1\n",
    "                pbar.set_postfix({'Iter':self.net1.iter,\n",
    "                                'Loss':'{0:.2e}'.format(loss.item())\n",
    "                                }) \n",
    "            print('Adam done!')\n",
    "            #这一块可能有问题，到时候再看\n",
    "            self.optimizer_LBFGS_u.step(self.LBFGS_epoch_loss)\n",
    "            self.optimizer_LBFGS_v.step(self.LBFGS_epoch_loss)\n",
    "            print('LBFGS done!')\n",
    "            \n",
    "            error_u, error_v = self.evaluate()\n",
    "            print('change_counts', move_count, 'Test_L2error_u:', '{0:.2e}'.format(error_u),'Test_L2error_v:', '{0:.2e}'.format(error_v))\n",
    "            self.x_test_estimate_collect.append([move_count, '{0:.2e}'.format(error_u),'{0:.2e}'.format(error_v)])\n",
    "\n",
    "            \n",
    "            if AM_type == 0: #基于残差，分配点向残差大的地方移动\n",
    "                x_init = random_fun(lb,ub,100000)\n",
    "                f1, f2 = self.x_f_loss_fun(x_init,self.train_U,self.train_V)\n",
    "                x_init_residual = abs(f1) + abs(f2)\n",
    "                x_init_residual = x_init_residual.cpu().detach().numpy()\n",
    "                err_eq = np.power(x_init_residual, AM_K) / np.power(x_init_residual, AM_K).mean()\n",
    "                err_eq_normalized = (err_eq / sum(err_eq))[:, 0]\n",
    "                X_ids = np.random.choice(a=len(x_init), size=M, replace=False, p=err_eq_normalized)\n",
    "                self.x_f_M = x_init[X_ids]\n",
    "                \n",
    "            elif AM_type == 1:\n",
    "                x_init = random_fun(lb,ub,100000)\n",
    "                x = Variable(x_init,requires_grad=True)\n",
    "                u = self.train_U(x)\n",
    "                v = self.train_V(x)\n",
    "                dux = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "                dvx = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "                grad_ux = dux[:[0]].squeeze()\n",
    "                grad_uy = dux[:[1]].squeeze()\n",
    "                grad_ut = dux[:[2]].squeeze()\n",
    "                grad_vx = dvx[:[0]].squeeze()\n",
    "                grad_vy = dvx[:[0]].squeeze()\n",
    "                grad_vt = dvx[:[0]].squeeze()\n",
    "                dx = torch.sqrt(1 + grad_ux ** 2 + grad_uy ** 2 + grad_ut ** 2\n",
    "                                + grad_vx ** 2 + grad_vy ** 2 + grad_vt ** 2).cpu().detach().numpy()\n",
    "                err_dx = np.power(dx, AM_K) / np.power(dx, AM_K).mean()\n",
    "                p = (err_dx / sum(err_dx))\n",
    "                X_ids = np.random.choice(a=len(x_init), size=M, replace=False, p=p)\n",
    "                self.x_f_M = x_init[X_ids]\n",
    "                \n",
    "    def run_AM_AW(self):\n",
    "        self.x_f_s = nn.Parameter(self.x_f_s, requires_grad=True)\n",
    "        self.x_ic_s = nn.Parameter(self.x_ic_s, requires_grad=True)\n",
    "        self.x_bc_s = nn.Parameter(self.x_bc_s, requires_grad=True)\n",
    "        for move_count in range(AM_count):\n",
    "            optimizer_adam_u = torch.optim.Adam(self.net1.parameters(), lr = adam_lr)\n",
    "            optimizer_adam_v = torch.optim.Adam(self.net2.parameters(), lr = adam_lr)\n",
    "            self.optimizer_LBFGS_u = torch.optim.LBFGS(self.net1.parameters(),lr = LBFGS_lr,\n",
    "                                                    max_iter=LBFGS_iter)\n",
    "            self.optimizer_LBFGS_v = torch.optim.LBFGS(self.net2.parameters(),lr = LBFGS_lr,\n",
    "                                                    max_iter=LBFGS_iter)\n",
    "            optimizer_adam_weight = torch.optim.Adam([self.x_f_s] + [self.x_ic_s] + [self.x_bc_s],\n",
    "                                                        lr=AW_lr)\n",
    "            pbar = trange(adam_iter, ncols=100)\n",
    "            for i in pbar:\n",
    "                self.s_collect.append([self.net1.iter, self.x_f_s.item(), self.x_ic_s.item(), self.x_bc_s.item()])\n",
    "                optimizer_adam_u.zero_grad()\n",
    "                optimizer_adam_v.zero_grad()\n",
    "                loss_e,loss_ic,loss_bc = self.epoch_loss()\n",
    "                loss = self.true_loss(loss_e, loss_ic, loss_bc)\n",
    "                loss.backward()\n",
    "                optimizer_adam_u.step()\n",
    "                optimizer_adam_v.step()\n",
    "                self.net1.iter += 1\n",
    "                self.net2.iter += 1\n",
    "                pbar.set_postfix({'Iter':self.net1.iter,\n",
    "                                'Loss':'{0:.2e}'.format(loss.item())\n",
    "                                }) \n",
    "                optimizer_adam_weight.zero_grad()\n",
    "                loss = self.likelihood_loss(loss_e, loss_ic, loss_bc)\n",
    "                loss.backward()\n",
    "                optimizer_adam_weight.step()\n",
    "                \n",
    "            print('Adam done!')\n",
    "            #这一块可能有问题，到时候再看\n",
    "            self.optimizer_LBFGS_u.step(self.LBFGS_epoch_loss)\n",
    "            self.optimizer_LBFGS_v.step(self.LBFGS_epoch_loss)\n",
    "            print('LBFGS done!')\n",
    "            \n",
    "            error_u, error_v = self.evaluate()\n",
    "            print('change_counts', move_count, 'Test_L2error_u:', '{0:.2e}'.format(error_u),'Test_L2error_v:', '{0:.2e}'.format(error_v))\n",
    "            self.x_test_estimate_collect.append([move_count, '{0:.2e}'.format(error_u),'{0:.2e}'.format(error_v)])\n",
    "            \n",
    "            if AM_type == 0: #基于残差，分配点向残差大的地方移动\n",
    "                x_init = random_fun(lb,ub,100000)\n",
    "                f1, f2 = self.x_f_loss_fun(x_init,self.train_U,self.train_V)\n",
    "                x_init_residual = abs(f1) + abs(f2)\n",
    "                x_init_residual = x_init_residual.cpu().detach().numpy()\n",
    "                err_eq = np.power(x_init_residual, AM_K) / np.power(x_init_residual, AM_K).mean()\n",
    "                err_eq_normalized = (err_eq / sum(err_eq))[:, 0]\n",
    "                X_ids = np.random.choice(a=len(x_init), size=M, replace=False, p=err_eq_normalized)\n",
    "                self.x_f_M = x_init[X_ids]\n",
    "                \n",
    "            elif AM_type == 1:\n",
    "                x_init = random_fun(lb,ub,100000)\n",
    "                x = Variable(x_init,requires_grad=True)\n",
    "                u = self.train_U(x)\n",
    "                v = self.train_V(x)\n",
    "                dux = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "                dvx = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "                grad_ux = dux[:[0]].squeeze()\n",
    "                grad_uy = dux[:[1]].squeeze()\n",
    "                grad_ut = dux[:[2]].squeeze()\n",
    "                grad_vx = dvx[:[0]].squeeze()\n",
    "                grad_vy = dvx[:[0]].squeeze()\n",
    "                grad_vt = dvx[:[0]].squeeze()\n",
    "                dx = torch.sqrt(1 + grad_ux ** 2 + grad_uy ** 2 + grad_ut ** 2\n",
    "                                + grad_vx ** 2 + grad_vy ** 2 + grad_vt ** 2).cpu().detach().numpy()\n",
    "                err_dx = np.power(dx, AM_K) / np.power(dx, AM_K).mean()\n",
    "                p = (err_dx / sum(err_dx))\n",
    "                X_ids = np.random.choice(a=len(x_init), size=M, replace=False, p=p)\n",
    "                self.x_f_M = x_init[X_ids]\n",
    "        \n",
    "    def train(self):\n",
    "        self.x_f_N = random_fun(lb = lb,ub = ub,num = N)\n",
    "        self.x_f_M = random_fun(lb = lb,ub = ub,num = M)\n",
    "        \n",
    "        self.x_f_s = is_cuda(torch.tensor(0.).float())\n",
    "        self.x_ic_s = is_cuda(torch.tensor(0.).float())\n",
    "        self.x_bc_s = is_cuda(torch.tensor(0.).float())\n",
    "        \n",
    "        start_time = time.time()\n",
    "        if model_type == 0:\n",
    "            self.run_baseline()\n",
    "        elif model_type == 1:\n",
    "            self.run_AM()\n",
    "        elif model_type == 2:\n",
    "            self.run_AM_AW()\n",
    "        elapsed = time.time() - start_time\n",
    "        print('Training time: %.2f' % elapsed)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_f_loss_fun(x, train_U, train_V):\n",
    "    if not x.requires_grad:\n",
    "        x = Variable(x, requires_grad=True)\n",
    "    u = train_U(x)\n",
    "    du = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)\n",
    "    u_x = du[0][:, 0].unsqueeze(-1)\n",
    "    u_y = du[0][:, 1].unsqueeze(-1)\n",
    "    u_t = du[0][:, 2].unsqueeze(-1)\n",
    "    d_ux = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)\n",
    "    u_xx = d_ux[0][:, 0].unsqueeze(-1)\n",
    "    d_uy = torch.autograd.grad(u_y, x, grad_outputs=torch.ones_like(u_x), create_graph=True)\n",
    "    u_yy = d_uy[0][:, 1].unsqueeze(-1)\n",
    "    \n",
    "    v = train_V(x)\n",
    "    dv = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)\n",
    "    v_x = dv[0][:, 0].unsqueeze(-1)\n",
    "    v_y = dv[0][:, 1].unsqueeze(-1)\n",
    "    v_t = dv[0][:, 2].unsqueeze(-1)\n",
    "    d_vx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)\n",
    "    v_xx = d_vx[0][:, 0].unsqueeze(-1)\n",
    "    d_vy = torch.autograd.grad(v_y, x, grad_outputs=torch.ones_like(v_y), create_graph=True)\n",
    "    v_yy = d_vy[0][:, 1].unsqueeze(-1)\n",
    "    \n",
    "    f1 = u_t - u * u * u_x - v * v * u_y\n",
    "    f2 = v_t - u * u * v_x - v * v * v_y\n",
    "    return f1,f2\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置参数\n",
    "layers = [2, 20, 20, 20, 20, 1]\n",
    "net1 = is_cuda(Net1(layers))\n",
    "net2 = is_cuda(Net2(layers))\n",
    "def exact_u(x):\n",
    "    return np.sin(x[:, [0]]+x[:, [1]]+x[:, [2]])\n",
    "def exact_v(x):\n",
    "    return np.cos(x[:, [0]]+x[:, [1]]+x[:, [2]])\n",
    "lb = np.array([0, 0, 0])\n",
    "ub = np.array([2*np.pi, 2*np.pi, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备数据\n",
    "#test data\n",
    "#解析解已知的情况\n",
    "P=2 #测试点个数P*P*P\n",
    "x = np.expand_dims(np.linspace(0, 2*np.pi, P), axis=1)\n",
    "y = np.expand_dims(np.linspace(0, 2*np.pi, P), axis=1)\n",
    "t = np.expand_dims(np.linspace(0, 1, P), axis=1)\n",
    "X, Y, T = np.meshgrid(x, y, t) #P*P*P网格\n",
    "x_test_np = np.concatenate((np.expand_dims(X.flatten(), axis=1), np.expand_dims(Y.flatten(), axis=1),np.expand_dims(T.flatten(), axis=1)), axis=-1)#(P*P*P,3)\n",
    "solution_u = exact_u(x_test_np)\n",
    "solution_v = exact_v(x_test_np)\n",
    "x_test = is_cuda(torch.from_numpy(x_test_np).float())\n",
    "x_test_exact_u = is_cuda(torch.from_numpy(solution_u).float())\n",
    "x_test_exact_v = is_cuda(torch.from_numpy(solution_v).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始数据\n",
    "Nic = 10 #初始数据个数\n",
    "x_init = torch.full([Nic, 2], -1) + torch.rand([Nic, 2]) * 2 #-1,1之间的随机数\n",
    "x_init = torch.rand([Nic, 2]) * 2 * np.pi #0,2pi之间的随机数\n",
    "# x_init = torch.rand([Nic, 2]) #0,1之间的随机数\n",
    "x_initial = torch.cat(( x_init, torch.zeros(Nic, 1)), dim=1)\n",
    "x_initial_u = torch.sin(x_initial[:, [0]] + x_initial[:, [1]])\n",
    "x_initial_v = torch.cos(x_initial[:, [0]] + x_initial[:, [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 边界数据    \n",
    "Nbc = 10 #边界数据个数 \n",
    "# # 左边界\n",
    "# x1_boundary_left = torch.cat((torch.full([Nbc, 1], -1), torch.full([Nbc, 1], -1) + torch.rand([Nbc, 1]) * 2, torch.rand([Nbc, 1])), dim=1)\n",
    "# # 右边界\n",
    "# x1_boundary_right = torch.cat((torch.full([Nbc, 1], 1), torch.full([Nbc, 1], -1) + torch.rand([Nbc, 1]) * 2, torch.rand([Nbc, 1])), dim=1)\n",
    "# # 下边界\n",
    "# x2_boundary_left = torch.cat((torch.full([Nbc, 1], -1) + torch.rand([Nbc, 1]) * 2, torch.full([Nbc, 1], -1), torch.rand([Nbc, 1])), dim=1)\n",
    "# # 上边界\n",
    "# x2_boundary_right = torch.cat((torch.full([Nbc, 1], -1) + torch.rand([Nbc, 1]) * 2, torch.full([Nbc, 1], 1), torch.rand([Nbc, 1])), dim=1)\n",
    "\n",
    "# 左边界\n",
    "x1_boundary_left = torch.cat((torch.full([Nbc, 1], 0), torch.rand([Nic, 1]) * 2 * np.pi, torch.rand([Nbc, 1])), dim=1)\n",
    "# 右边界\n",
    "x1_boundary_right = torch.cat((torch.full([Nbc, 1], 2 * np.pi), torch.rand([Nic, 1]) * 2 * np.pi, torch.rand([Nbc, 1])), dim=1)\n",
    "# 下边界\n",
    "x2_boundary_left = torch.cat((torch.rand([Nic, 1]) * 2 * np.pi, torch.full([Nbc, 1], 0), torch.rand([Nbc, 1])), dim=1)\n",
    "# 上边界\n",
    "x2_boundary_right = torch.cat((torch.rand([Nic, 1]) * 2 * np.pi, torch.full([Nbc, 1], 2 * np.pi), torch.rand([Nbc, 1])), dim=1)\n",
    "\n",
    "#真实值\n",
    "x1_boundary_left_label_u = torch.from_numpy(\n",
    "        exact_u(x1_boundary_left.numpy())).float()\n",
    "x1_boundary_right_label_u = torch.from_numpy(\n",
    "        exact_u(x1_boundary_right.numpy())).float()\n",
    "x2_boundary_left_label_u = torch.from_numpy(\n",
    "        exact_u(x2_boundary_left.numpy())).float()\n",
    "x2_boundary_right_label_u = torch.from_numpy(\n",
    "        exact_u(x2_boundary_right.numpy())).float()\n",
    "x1_boundary_left_label_v = torch.from_numpy(\n",
    "        exact_v(x1_boundary_left.numpy())).float()\n",
    "x1_boundary_right_label_v = torch.from_numpy(\n",
    "        exact_v(x1_boundary_right.numpy())).float()\n",
    "x2_boundary_left_label_v = torch.from_numpy(\n",
    "        exact_v(x2_boundary_left.numpy())).float()\n",
    "x2_boundary_right_label_v = torch.from_numpy(\n",
    "        exact_v(x2_boundary_right.numpy())).float()\n",
    "\n",
    "# x1_boundary_left_label_u = torch.zeros([Nbc, 1])\n",
    "# x1_boundary_right_label_u = torch.zeros([Nbc, 1])\n",
    "# x2_boundary_left_label_u = torch.zeros([Nbc, 1])\n",
    "# x2_boundary_right_label_u = torch.zeros([Nbc, 1])\n",
    "# x1_boundary_left_label_v = torch.zeros([Nbc, 1])\n",
    "# x1_boundary_right_label_v = torch.zeros([Nbc, 1])\n",
    "# x2_boundary_left_label_v = torch.zeros([Nbc, 1])\n",
    "# x2_boundary_right_label_v = torch.zeros([Nbc, 1])\n",
    "\n",
    "x_ic = is_cuda(x_initial)\n",
    "u_ic = is_cuda(x_initial_u)\n",
    "v_ic = is_cuda(x_initial_v)\n",
    "x_bc = is_cuda(torch.cat((x1_boundary_left, x1_boundary_right, x2_boundary_left, x2_boundary_right), dim=0))\n",
    "u_bc = is_cuda(torch.cat((x1_boundary_left_label_u, x1_boundary_right_label_u,x2_boundary_left_label_u, x2_boundary_right_label_u), dim=0))\n",
    "v_bc = is_cuda(torch.cat((x1_boundary_left_label_v, x1_boundary_right_label_v,x2_boundary_left_label_v, x2_boundary_right_label_v), dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(net1,net2,x_bc,u_bc,v_bc,x_ic,u_ic,v_ic,\n",
    "                 x_f_loss_fun,x_test,x_test_exact_u,x_test_exact_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                      | 0/5000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2000x3 and 2x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mf:\\研二上\\Enhancing PINNs\\AMAW-PINN-master\\2Dpossion_equation\\test_withtime.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain()\n",
      "\u001b[1;32mf:\\研二上\\Enhancing PINNs\\AMAW-PINN-master\\2Dpossion_equation\\test_withtime.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=269'>270</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_AM()\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m \u001b[39melif\u001b[39;00m model_type \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=271'>272</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_AM_AW()\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=272'>273</a>\u001b[0m elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=273'>274</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining time: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m elapsed)\n",
      "\u001b[1;32mf:\\研二上\\Enhancing PINNs\\AMAW-PINN-master\\2Dpossion_equation\\test_withtime.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m optimizer_adam_u\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m optimizer_adam_v\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m loss_e,loss_ic,loss_bc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loss()\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrue_loss(loss_e, loss_ic, loss_bc)\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=204'>205</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32mf:\\研二上\\Enhancing PINNs\\AMAW-PINN-master\\2Dpossion_equation\\test_withtime.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mepoch_loss\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     x_f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_f_N,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_f_M), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     f1, f2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_f_loss_fun(x_f,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_U,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_V)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     loss_equation \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(f1 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m f2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     loss_ic \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_U(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_ic) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu_ic) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_V(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_ic) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_ic) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "\u001b[1;32mf:\\研二上\\Enhancing PINNs\\AMAW-PINN-master\\2Dpossion_equation\\test_withtime.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m x\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     x \u001b[39m=\u001b[39m Variable(x, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m u \u001b[39m=\u001b[39m train_U(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m du \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(u, x, grad_outputs\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mones_like(u), create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m u_x \u001b[39m=\u001b[39m du[\u001b[39m0\u001b[39m][:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mf:\\研二上\\Enhancing PINNs\\AMAW-PINN-master\\2Dpossion_equation\\test_withtime.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_U\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet1(x)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mf:\\研二上\\Enhancing PINNs\\AMAW-PINN-master\\2Dpossion_equation\\test_withtime.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear[\u001b[39m0\u001b[39;49m](x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers) \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E7%A0%94%E4%BA%8C%E4%B8%8A/Enhancing%20PINNs/AMAW-PINN-master/2Dpossion_equation/test_withtime.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear[i](a)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2000x3 and 2x20)"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
